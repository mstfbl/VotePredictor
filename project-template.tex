\documentclass[11pt]{article}
\usepackage{common}
\usepackage{float}
\usepackage{algorithm,algpseudocode}

\title{Using Naive Bayes to Predict Votes of Congressmen from Bill Texts}
\author{David Gibson, Thomas Chang, Mustafa Bal}
\begin{document}
\maketitle{}


\section{Introduction}

The problem we set out to solve was simple: given the text of a bill proposed in Congress, design an algorithm to predict whether it will pass or not. Such an algorithm could prove useful to many people, especially because using an algorithmic approach attempts to adress the problem in ways largely different from conventional approaches. The most common way to solve this problem is simply taking the predictions of political experts. While these predictions are usually good, algorithms take a completely different approach towards solving the problem, and could potentially reveal intricacies to the problem that are missed using the conventional method of human predictions. A best potential model might incorporate both human and computer predictions, since both offer different ways of looking at the same problem.

There have been several attempts to solve this particular classification problem through algorithms. A Harvard team devised an algorithm based on using word embeddings to model congressmen as ideal vectors in higher dimensional space. Their model took into account multivariate relationships between words to do predictions. We decided to use a similar approach, with several simplifications. Rather than using a word embedding system, we decided to take the naive Bayes approach discussed in class, adapted to the problem at hand. The naive Bayes approach seemed suitable for this problem due to its similarity to the naive Bayes examples raised in class, such as the spam email filter. Both are similar document classification tasks that make sense to solve using naive Bayes, because presumably there are noticeable differences in text between words that each congressman votes yes or no for. Using the naive Bayes approach on texts this large raised several interesting implementation problems, however. Because bills themselves are so long, with the largest one in our data set being over 250 pages, it raises the question of which words to consider in the naive Bayes analysis. One approach would be to simply use each word as a separate feature, but with such large texts other solutions are possible. We experimented with document frequency algorithms to identify key words in bills with which to run our naive Bayes algorithm on, ignoring unimportant words such as "the" or "and". How many words to include in our analysis was a key hyperparameter of our algorithm we had to tune.

\iffalse
A description of the purpose, goals, and scope of your system or
empirical investigation.  You should include references to papers you
read on which your project and any algorithms you used are
based. Include a discussion of whether you adapted a published
algorithm or devised a new one, the range of problems and issues you
addressed, and the relation of these problems and issues to the
techniques and ideas covered in the course.
\fi 

\section{Background and Related Work}

For instance, \cite{hochreiter1997long}.


\section{Problem Specification}

In simplest terms, the problem in question is this: given the text of a congressional bill, predict whether that bill will pass or fail. We decided to extend this problem to posing a similar, related problem: given the text of a congressional bill, predict the specific vote of each congressman on that bill. Solving this problem allowed us to use the generated vote distribution to give specific yes/no/abstain vote counts on each predicted bill and individual vote patterns in addition to simply answering the question of predicting whether the bill will pass or not. In formal terms, the input to our algorithm is simply the text of a potential bill, and the output is the voting predictions of each congressman and an overall prediction on whether or not the bill will pass.

\iffalse
A clear description of the problem you are solving in both general terms 
and how you've mapped it to a formal problem specification. 
\fi


\section{Approach}

The algorithm we used to classify congressmen and bills uses a naive Bayes approach based on the words present in each bill. Given a set of labeled data mapping bill texts to congressman votes, the algorithm first trains on the set. We allocated 90\% of our data to train the algorithm, and 10\% to verify the success of the algorithm. To train the model, we used the simple naive Bayes text classification approach discussed in class. Because we are predicting votes for each congressman, our approach actually consists of solving about 600 individual sub-classification problems, one for each congressman. For each single congressman prediction problem, each bill has a feature for every word, and each feature can take on the value of yes/no/abstain. In accordance with the naive Bayes bag of words model, for each congressman we kept track of three Python dictionaries, one for each of yes/no/abstain. Each dictionary maps words to the total count of the number of that specific word in the entire corpus of all bills labeled with the respective vote predictor label. Once we build these up through training, we can divide these by the total number of words in all the documents classified as yes/no/abstain to generate the general probability that each word occurs in bill labeled as a yes/no/abstain for each specific congressman. This is contained in algorithm 1.

Once we train the model as such, it becomes simple to generate a predictive label for a general bill. We simply iterate through each word in the bill and keep track of the probability that the entire string of words comprising the bill comes from a yes bill, no bill, or abstain bill for each congressman. At the end fo the document, we simply classify the document according to whichever probability is highest. Because these probabilities become very small, we work in terms of log(probability) instead. We repeat this process for each congressman, using the respective congressman's unique probability distribution table. This is contained in algorithm 4.

There were several model hyperparameters present in the model we used. For words not present in the training set at all, assigning them a probability of 0 would warp our model significantly. As soon as such a word is seen in a bill to be predicted, the probability immediately drops to 0 and remains there. To address this, we applied Laplacian smoothing, which pretends we see each outcome a constant amount of additional times. Another hyperparameter to tune was the number of words to consider as part of the naive Bayes likelihood function. The basic implementation considers every word as a feature, but our hyperparameter defined a certain number of words to use instead of the full document. We generated these words using a numerical statistic known as tf-idf. This stands for term frequency-inverse document frequency, and is a way to find the most common words in a document, controlling for their frequency in a set of documents. We generated tf-idf scores for each word in bills to be predicted, using the set of all bills as the corpus to draw generic prior frequencies from. This allowed us to find the $k$ most abnormally present words in each document, and only consider them as features for the naive Bayes likelihood function. The process of calculating the tf-idf scores of words is contained in algorithms 2 and 3.

\begin{algorithm}
	\begin{algorithmic}
		\Require training set
		\State initialize yes, no, abstain dictionary with default value 0
		\State initialize yeswords, nowords, abstainwords
		\For {bill in training set}
		\State vote = actual vote result
		\For {word in bill}
		\If {vote == yes}
		\State yes[word] += 1
		\State yescount += 1
		\EndIf
		\If {vote == no}
		\State no[word] += 1
		\State nocount += 1
		\EndIf
		\If {vote == abstain}
		\State abstain[word] += 1
		\State abstaincount += 1
		\EndIf
		\EndFor
		\EndFor
		\State normalize all words in each dictionary by total count of words with that yes/no/abstain classification
		
		\noindent \Return yes, no, abstain
	\end{algorithmic}
	\caption{Generate Congressman Word Probability Distribution}
\end{algorithm}

\begin{algorithm} [H]
	\begin{algorithmic}
	\Require training set
	\State initialize idf dictionary, document\_count
	\For {bill in training set}
		\For {unique word in bill}
			\State idf[word] += 1							
		\EndFor
		\State document\_count += 1
	\EndFor
	\For {word in idf}
		\State idf[word] = $\log$(document\_count / idf[word])
	\EndFor
	
	\noindent \Return idf
	\end{algorithmic}
	\caption{generate idf values}
\end{algorithm}

\begin{algorithm} [H]
	\begin{algorithmic}
		\Require idf, k, bill
		\State initialize tf dictionary, word\_count, tf-idf dictionary
		\For {word in training set}
			\State tf[word] += 1							
		\EndFor
		\For {word in tf}
			\State tf[word] = tf[word] / word\_count
		\EndFor
		\For{unique word in bill}
			\State tf-idf[word] = tf[word] * idf[word]
		\EndFor
									
		
		\noindent \Return $k$ words with highest tf-idf value
	\end{algorithmic}
	\caption{calculate $k$ words with highest tf-idf scores from bill}
\end{algorithm}


\begin{algorithm} [H]
	\begin{algorithmic}
	\Require bill, yes no abstain dictionaries, $k$ highest tf-idf words
	\State initialize p\_yes, p\_no, p\_abstain
	\For {word in bill}
		\If {word in $k$ highest tf-idf words}
			\State p\_yes += $\log$(yes[word])
			\State p\_no += $\log$(no[word])
			\State p\_abstain += $\log$(abstain[word])
		\EndIf
		\EndFor
	\State label bill as max(p\_yes, p\_no, p\_abstain)
	
	\noindent \Return yes, no, abstain
	\end{algorithmic}
	\caption{Predict Congressman Vote on Bill}
\end{algorithm}

\iffalse
A clear specification of the algorithm(s) you used and a description
of the main data structures in the implementation. Include a
discussion of any details of the algorithm that were not in the
published paper(s) that formed the basis of your implementation. A
reader should be able to reconstruct and verify your work from reading
your paper.

\begin{algorithm}
  \begin{algorithmic}
    \Procedure{MyAlgorithm}{$b$}
    \State{$a \gets 10$}
    \EndProcedure{}
  \end{algorithmic}
  \caption{Here is the algorithm.}
\end{algorithm}
\fi


\section{Experiments}

We tested our algorithm over many different values of $k$ for the tf-idf hyperparameter as well as $k=\infty$ which is equivalent to ignoring tf-idf and simply using naive Bayes over all words. We also compared it to several baselines. Our 3 baselines were predicting every congressman as yes, predicting every congressman as no, and predicting every congressman using a weighted coinflip based on the total number of yes/no/abstains in the entire training set. There were two relevant test statistics we calculated for each approach: mean congressman success probability, and overall bill prediction success rates. Although mean congressman success probability is correlated with overall bill prediction success rates, it is not completely indicative of the overall success rate. Both statistics are useful, and so we use both to summarize the effectiveness of our algorithm.

\iffalse
Analysis, evaluation, and critique of the algorithm and your
implementation. Include a description of the testing data you used and
a discussion of examples that illustrate major features of your
system. Testing is a critical part of system construction, and the
scope of your testing will be an important component in our
evaluation. Discuss what you learned from the implementation.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    & Score \\
    \midrule
    Approach 1 & \\
    Approach 2 & \\
    \bottomrule
  \end{tabular}
  \caption{Description of the results.}
\end{table}

\fi


\subsection{Results}


\section{Discussion}

One flaw to our approach was the underlying naive Bayes assumption that each feature is independent of all others. This is a general simplifying assumption that allowed us to actually implement an algorithm and get results, but in general could be reevaluated for better performance. When dealing with bills, it is likely that looking at longer pieces of text, like pairs of words or sentences, would provide better results. Interactions between words are key to their meaning, so expanding our model to incorporate those would be an improvement to consider if we were to expand our project to a larger scope. Given more time, we could also test several more hyperparameters for our model. We could have implemented a td-idf based document keyword algorithm as part of our training set, for example. Although we are unsure what the effect of such a change would be, it is possible that it could improve our classifier.

\iffalse
Summary of approach and results. Major takeaways? Things you could improve in future work?
\fi

\appendix

\section{System Description}

 Appendix 1 – A clear description of how to use your system and how to generate the output you discussed in the write-up. \emph{The teaching staff must be able to run your system.}

\section{Group Makeup}

 Appendix 2 – A list of each project participant and that
participant’s contributions to the project. If the division of work
varies significantly from the project proposal, provide a brief
explanation.  Your code should be clearly documented. 



\bibliographystyle{plain} 
\bibliography{project-template}

\end{document}
